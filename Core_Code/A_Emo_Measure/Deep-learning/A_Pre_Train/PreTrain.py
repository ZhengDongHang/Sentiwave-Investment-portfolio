import os
import json
import random
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForPreTraining
from torch.optim import AdamW
from tqdm import tqdm

from MLM import get_mlm_collator
from NSP import build_nsp_sentence_pairs
from Loss_Record import loss_record
from Data_Clean import clean_dataset

from transformers import logging
logging.set_verbosity_error() # é™é»˜è­¦å‘Š

DATA_DIR = "../../data/train_data"
SAVE_PATH = "../model/pre_train_model"
MODEL_NAME = "/data/public/fintechlab/zdh/Individual-Stock-Analysis/A_Emo_Measure/bert-base-chinese"
loss_log_path = "../loss/PreTrain_loss.csv"
BATCH_SIZE = 1024
EPOCHS = 3
MAX_LEN = 64

# æ£€æŸ¥æŸå¤±æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆ é™¤ï¼Œé˜²æ­¢é‡å¤å†™å…¥
if os.path.exists(loss_log_path):
    os.remove(loss_log_path)
    print("åŸæŸå¤±æ–‡ä»¶å·²åˆ é™¤")

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device_ids = list(range(8)) # ä½¿ç”¨8å¼ 3090æ˜¾å¡
os.makedirs(SAVE_PATH, exist_ok=True)


tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
model = BertForPreTraining.from_pretrained(MODEL_NAME)
model = torch.nn.DataParallel(model, device_ids=device_ids, output_device=1).to(DEVICE)

# è‡ªå®šä¹‰ Dataset
class BertPretrainDataset(Dataset):
    def __init__(self, sentence_pairs, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.sentence_pairs = sentence_pairs

    def __len__(self):
        return len(self.sentence_pairs)

    def __getitem__(self, idx):
        text_a, text_b, label = self.sentence_pairs[idx]
        encoding = self.tokenizer.encode_plus(
                        text_a,
                        text_b,
                        max_length=self.max_len,
                        padding='max_length',
                        truncation=True,
                        return_overflowing_tokens=False,
                        return_tensors='pt'
                    )
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'token_type_ids': encoding['token_type_ids'].squeeze(0),
            'next_sentence_label': torch.tensor(label, dtype=torch.long)
        }

# åŠ è½½æ•°æ®
def load_all_data(data_dir):
    all_data = []
    for filename in os.listdir(data_dir):
        if filename.endswith(".json"):
            with open(os.path.join(data_dir, filename), "r", encoding="utf-8") as f:
                try:
                    all_data.extend(json.load(f))
                except json.JSONDecodeError:
                    print(f"âš ï¸ è·³è¿‡æ— æ³•è§£æçš„æ–‡ä»¶ï¼š{filename}")

    subset = all_data[:]  # *** å–ä¸€ä¸ªå­é›†ï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰*** #
    random.shuffle(subset)
    return subset

print("ğŸŸ¢ æ­£åœ¨åŠ è½½å…¨é‡æ•°æ®ä¸­...")
all_json_data = load_all_data(DATA_DIR) # åŠ è½½æ•°æ®
print("âœ… åŠ è½½å…¨é‡æ•°æ®æˆåŠŸ")

print("ğŸŸ¢ æ­£åœ¨è¿›è¡Œæ•°æ®æ¸…æ´—æ“ä½œ...")
all_json_data = clean_dataset(all_json_data) # æ¸…æ´—æ•°æ®
print("âœ… æ•°æ®æ¸…æ´—æˆåŠŸ")

print("ğŸŸ¢ æ­£åœ¨è¿›è¡ŒNSPæ–‡æœ¬å¯¹çš„æ„å»º...")
sentence_pairs = build_nsp_sentence_pairs(all_json_data) # NSPæ•°æ®æ„å»º
print("âœ… NSPæ–‡æœ¬å¯¹çš„æ„å»ºæˆåŠŸ")

print("ğŸŸ¢ æ­£åœ¨è¿›è¡Œæ•°æ®è½¬åŒ–...")
dataset = BertPretrainDataset(sentence_pairs, tokenizer, MAX_LEN)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
print("âœ… æ•°æ®è½¬åŒ–æˆåŠŸ")

collator = get_mlm_collator(tokenizer)
optimizer = AdamW(model.parameters(), lr=1e-5)

model.train()
print("ğŸŸ¢ å¼€å§‹è®­ç»ƒ...")
for epoch in range(EPOCHS):
    print(f"\nEpoch {epoch + 1}/{EPOCHS}")
    loop = tqdm(dataloader, leave=True)
    for batch in loop:
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        token_type_ids = batch['token_type_ids'].to(DEVICE)
        next_sentence_label = batch['next_sentence_label'].to(DEVICE)

        mlm_inputs = collator([
            {'input_ids': i, 'attention_mask': a}
            for i, a in zip(input_ids, attention_mask)
        ])
        mlm_inputs['token_type_ids'] = token_type_ids
        mlm_inputs['next_sentence_label'] = next_sentence_label
        mlm_inputs = {k: v.to(DEVICE) for k, v in mlm_inputs.items()}

        outputs = model(**mlm_inputs)
        loss = outputs.loss.mean()
        loss_record(loss_log_path, epoch, loop, loss) # æŸå¤±è®°å½•

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        loop.set_description(f"Epoch [{epoch + 1}/{EPOCHS}]")
        loop.set_postfix(loss=loss.item())

model.module.save_pretrained(SAVE_PATH)
tokenizer.save_pretrained(SAVE_PATH)
print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{SAVE_PATH}")
