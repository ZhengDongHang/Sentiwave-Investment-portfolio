import os
import joblib
import numpy as np
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LinearRegression

from utils import load_all_data  # ğŸ”„ ä½¿ç”¨å°è£…å¥½çš„è¯»å–é€»è¾‘

# ---------------- å‚æ•°é…ç½® ----------------
folder_path = '../../data/train_data/'  # è®­ç»ƒé›†jsonæ–‡ä»¶å¤¹è·¯å¾„
save_model_path = 'linear_model.pkl'    # æ¨¡å‹ä¿å­˜è·¯å¾„
max_len = 64                            # æœ€å¤§æ–‡æœ¬é•¿åº¦

# ---------------- åŠ è½½ + æ¸…æ´—æ•°æ® ----------------
print("ğŸ”„ åŠ è½½è®­ç»ƒæ•°æ®...")
all_data = load_all_data(folder_path, if_train=True)

# ---------------- ç”Ÿæˆè®­ç»ƒæ ·æœ¬ ----------------
print("ğŸ”¨ å¤„ç†è®­ç»ƒæ ·æœ¬...")
texts = []
targets = []

for item in tqdm(all_data, desc="æ„å»ºæ ·æœ¬"):
    index = item['index']
    for text in item.get('guba_data', []):
        if len(text) > max_len:
            text = text[:max_len]  # æˆªæ–­æ–‡æœ¬
        texts.append(text)
        targets.append(index)

targets = np.array(targets)

# ---------------- ç‰¹å¾æå– ----------------
print("ğŸ” æå–ç‰¹å¾...")
vectorizer = TfidfVectorizer(max_features=5000)
X_text = vectorizer.fit_transform(texts)

X = X_text  # ä»…ä½¿ç”¨æ–‡æœ¬ç‰¹å¾ï¼Œæ— éœ€æ‹¼æ¥æ¥æº

# ---------------- æ¨¡å‹è®­ç»ƒ ----------------
print("ğŸ‹ï¸ æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
model = LinearRegression()
model.fit(X, targets)

# ---------------- ä¿å­˜æ¨¡å‹ ----------------
print("ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°", save_model_path)
joblib.dump((vectorizer, model), save_model_path)

print("âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜ã€‚")
