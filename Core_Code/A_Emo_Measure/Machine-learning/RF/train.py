import os
import joblib
import numpy as np
from tqdm import tqdm
from scipy.sparse import csr_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestRegressor
from utils import load_all_data  # ğŸ”„ å¯¼å…¥ç»Ÿä¸€çš„æ•°æ®åŠ è½½æ¨¡å—

# ---------------- å‚æ•°é…ç½® ----------------
folder_path     = '../../data/train_data/'   # JSON æ–‡ä»¶å¤¹è·¯å¾„
save_model_path = 'rf_model.pkl'             # æ¨¡å‹ä¿å­˜è·¯å¾„
max_features    = 5000                        # TF-IDF æœ€å¤§ç‰¹å¾æ•°
n_estimators    = 50                          # éšæœºæ£®æ—ä¸­æ ‘çš„æ•°é‡
max_depth       = None                        # æœ€å¤§æ·±åº¦ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
max_len         = 64                          # æœ€å¤§æ–‡æœ¬é•¿åº¦

# ---------------- 1. åŠ è½½å¹¶ç»„ç»‡æ•°æ® ----------------
print("ğŸ“¦ åŠ è½½å¹¶æ¸…æ´—æ•°æ®...")
data_list = load_all_data(folder_path, if_train=True)

texts = []
labels = []

for item in data_list:
    idx_vals = item.get("index", None)
    if idx_vals is None:
        continue

    for text in item.get("guba_data", []):
        if len(text) > max_len:
            text = text[:max_len]  # æˆªæ–­æ–‡æœ¬
        texts.append(text)
        labels.append(idx_vals)

labels = np.array(labels)
print(f"âœ… æ ·æœ¬æ•°ï¼š{len(texts)}ï¼Œæ ‡ç­¾ç»´åº¦ï¼š{labels.shape}")

# ---------------- 2. ç‰¹å¾æå– ----------------
print("ğŸ” æå– TF-IDF ç‰¹å¾...")
vectorizer = TfidfVectorizer(max_features=max_features)
X_train = vectorizer.fit_transform(texts)

# ---------------- 3. éšæœºæ£®æ—è®­ç»ƒ ----------------
print("ğŸŒ² è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
n_outputs = labels.shape[1]
estimators = []

for i in tqdm(range(n_outputs), desc="è®­ç»ƒç»´åº¦", unit="dim"):
    y_i = labels[:, i]
    rf = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        n_jobs=-1,
        random_state=42
    )
    rf.fit(X_train, y_i)
    estimators.append(rf)

# ---------------- 4. ä¿å­˜æ¨¡å‹ ----------------
os.makedirs(os.path.dirname(save_model_path) or '.', exist_ok=True)
joblib.dump((vectorizer, estimators), save_model_path)
print(f"âœ… æ¨¡å‹ä¿å­˜è‡³ {save_model_path}")
